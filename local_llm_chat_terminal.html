<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local LLM Chat Terminal (WebAssembly)</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script type="module">
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";
        window.webllm = webllm;
    </script>
</head>
<body>
    <div id="root"></div>
    
    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        function App() {
            const [modelLoading, setModelLoading] = useState(false);
            const [modelReady, setModelReady] = useState(false);
            const [status, setStatus] = useState('Click "Initialize LLM" to download and load the model');
            const [downloadProgress, setDownloadProgress] = useState('');
            const [chatHistory, setChatHistory] = useState([]);
            const [userInput, setUserInput] = useState('');
            
            const engineRef = useRef(null);
            const terminalRef = useRef(null);
            const [selectedModel, setSelectedModel] = useState('Llama-3.1-8B-Instruct-q4f32_1-MLC');

            // Available models (smaller ones load faster)
            const availableModels = [
                { id: 'Llama-3.1-8B-Instruct-q4f32_1-MLC', name: 'Llama 3.1 8B (4.3GB - Recommended)', size: '4.3GB' },
                { id: 'Llama-3.2-3B-Instruct-q4f16_1-MLC', name: 'Llama 3.2 3B (1.9GB - Faster)', size: '1.9GB' },
                { id: 'Llama-3.2-1B-Instruct-q4f16_1-MLC', name: 'Llama 3.2 1B (0.5GB - Fastest)', size: '0.5GB' },
                { id: 'Phi-3.5-mini-instruct-q4f16_1-MLC', name: 'Phi-3.5 Mini (2.2GB)', size: '2.2GB' },
            ];

            const initializeLLM = async () => {
                setModelLoading(true);
                setStatus('Initializing LLM engine...');
                
                try {
                    const initProgressCallback = (report) => {
                        setDownloadProgress(report.text);
                        setStatus(report.text);
                    };

                    const engine = await window.webllm.CreateMLCEngine(
                        selectedModel,
                        { 
                            initProgressCallback: initProgressCallback,
                        }
                    );
                    
                    engineRef.current = engine;
                    setModelReady(true);
                    setStatus('‚úÖ LLM ready! Model loaded and running locally in your browser.');
                    setDownloadProgress('');
                } catch (error) {
                    setStatus(`‚ùå Error loading model: ${error.message}`);
                    console.error(error);
                } finally {
                    setModelLoading(false);
                }
            };
            
            const handleUserInput = async (e) => {
                if (e.key !== 'Enter') return;
                
                const userMessage = userInput.trim();
                if (!userMessage) return;
                
                setChatHistory(prev => [...prev, { role: 'user', content: userMessage }]);
                setUserInput('');

                if (!modelReady) {
                    alert('Please initialize the LLM first!');
                    return;
                }

                try {
                    const reply = await engineRef.current.chat.completions.create({
                        messages: chatHistory.concat({ role: 'user', content: userMessage }),
                        temperature: 0.7,
                        max_tokens: 150,
                    });

                    const assistantReply = reply.choices[0].message.content.trim();
                    setChatHistory(prev => [...prev, { role: 'assistant', content: assistantReply }]);
                } catch (error) {
                    console.error('Error generating chat response:', error);
                }

                terminalRef.current.scrollTop = terminalRef.current.scrollHeight;
            };

            return (
                <div className="min-h-screen bg-gradient-to-br from-purple-50 via-blue-50 to-indigo-100 p-8">
                    <div className="max-w-7xl mx-auto">
                        {/* Header */}
                        <div className="bg-white rounded-lg shadow-xl p-8 mb-6">
                            <div className="flex items-center justify-between mb-4">
                                <div>
                                    <h1 className="text-4xl font-bold text-gray-800 mb-2">
                                        üß† Local LLM Chat Terminal
                                    </h1>
                                    <p className="text-gray-600">
                                        Chat with an AI running 100% in your browser via WebAssembly
                                    </p>
                                </div>
                                <div className="text-right">
                                    <div className="inline-block bg-green-100 text-green-800 px-4 py-2 rounded-full text-sm font-semibold">
                                        üîí 100% Local ‚Ä¢ No Cloud
                                    </div>
                                </div>
                            </div>
                        </div>

                        {/* Model Initialization */}
                        <div className="bg-white rounded-lg shadow-xl p-8 mb-6">
                            <h2 className="text-2xl font-bold text-gray-800 mb-4">1. Initialize Local LLM</h2>
                            
                            <div className="mb-4">
                                <label className="block text-sm font-semibold text-gray-700 mb-2">
                                    Select Model:
                                </label>
                                <select 
                                    value={selectedModel}
                                    onChange={(e) => setSelectedModel(e.target.value)}
                                    disabled={modelReady || modelLoading}
                                    className="w-full p-3 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 disabled:bg-gray-100"
                                >
                                    {availableModels.map(model => (
                                        <option key={model.id} value={model.id}>
                                            {model.name}
                                        </option>
                                    ))}
                                </select>
                                <p className="text-sm text-gray-500 mt-2">
                                    ‚ö†Ô∏è Model will be downloaded and cached in your browser. Choose smaller models for faster loading.
                                </p>
                            </div>

                            <button
                                onClick={initializeLLM}
                                disabled={modelLoading || modelReady}
                                className="bg-purple-600 text-white px-6 py-3 rounded-lg font-semibold hover:bg-purple-700 disabled:bg-gray-400 disabled:cursor-not-allowed transition"
                            >
                                {modelLoading ? '‚è≥ Loading Model...' : modelReady ? '‚úÖ Model Ready' : 'üöÄ Initialize LLM'}
                            </button>

                            {downloadProgress && (
                                <div className="mt-4 p-4 bg-blue-50 rounded-lg">
                                    <p className="text-sm text-blue-800 font-mono">{downloadProgress}</p>
                                </div>
                            )}

                            {modelReady && (
                                <div className="mt-4 p-4 bg-green-50 rounded-lg">
                                    <p className="text-green-800 font-semibold">
                                        ‚úÖ LLM is ready! The model is now running locally in your browser.
                                    </p>
                                </div>
                            )}
                        </div>

                        {/* Chat Terminal */}
                        <div className="bg-white rounded-lg shadow-xl p-8 mb-6">
                            <h2 className="text-2xl font-bold text-gray-800 mb-4">2. Chat with Local LLM</h2>

                            <div className="border border-gray-300 rounded-lg p-4 mb-4 h-96 overflow-y-auto" ref={terminalRef}>
                                {chatHistory.map((msg, idx) => (
                                    <div key={idx} className={`mb-2 ${msg.role === 'user' ? 'text-right' : ''}`}>
                                        <span className={`inline-block px-3 py-1 rounded-full text-sm font-semibold ${msg.role === 'user' ? 'bg-blue-500 text-white' : 'bg-gray-100 text-gray-800'}`}>
                                            {msg.role === 'user' ? 'You' : 'Assistant'}
                                        </span>
                                        <div className={`mt-1 text-gray-700 ${msg.role === 'user' ? 'text-right' : ''}`}>
                                            {msg.content}
                                        </div>
                                    </div>
                                ))}
                            </div>

                            <input
                                type="text"
                                value={userInput}
                                onChange={(e) => setUserInput(e.target.value)}
                                onKeyPress={handleUserInput}
                                placeholder="Type your message and press Enter..."
                                className="w-full p-3 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500"
                            />
                        </div>

                        {/* Status */}
                        <div className="bg-gradient-to-r from-blue-600 to-purple-600 text-white rounded-lg shadow-xl p-6 mb-6">
                            <div className="flex items-center">
                                <div className="text-3xl mr-4">
                                    {modelLoading ? '‚è≥' : modelReady ? '‚úÖ' : 'üí§'}
                                </div>
                                <div>
                                    <p className="font-semibold text-lg">{status}</p>
                                </div>
                            </div>
                        </div>

                        {/* Info Footer */}
                        <div className="mt-8 bg-gradient-to-r from-indigo-50 to-purple-50 rounded-lg p-6 border-2 border-indigo-200">
                            <h3 className="font-bold text-gray-800 mb-2">‚ÑπÔ∏è How It Works:</h3>
                            <ul className="text-sm text-gray-700 space-y-1">
                                <li>‚Ä¢ <strong>100% Local:</strong> The LLM runs entirely in your browser using WebAssembly + WebGPU</li>
                                <li>‚Ä¢ <strong>Privacy First:</strong> No data sent to any server - everything stays on your machine</li>
                                <li>‚Ä¢ <strong>One-Time Download:</strong> Model is cached in your browser after first load</li>
                                <li>‚Ä¢ <strong>Works Offline:</strong> After initial model download, works without internet</li>
                                <li>‚Ä¢ <strong>Powered by:</strong> Web LLM (MLC), running Llama models compiled to WebAssembly</li>
                            </ul>
                        </div>
                    </div>
                </div>
            );
        }

        ReactDOM.render(<App />, document.getElementById('root'));
    </script>
</body>
</html>
